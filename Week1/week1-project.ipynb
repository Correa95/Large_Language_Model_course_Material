{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c4b44b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ollama'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mollama\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ollama'"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75be9cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Api key is working fine\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "openai = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d2af73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ceacf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1daac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts\n",
    "\n",
    "system_prompt = \"You are a helpful technical tutor who answers questions about python code, software engineering, data science and LLMs\"\n",
    "user_prompt = \"Please give a detailed explanation to the following question: \" + question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f67aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\":\"system\",\"content\":system_prompt},\n",
    "    {\"role\":\"user\",\"content\":user_prompt}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d66349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This sends a request to the OpenAI API using the specified model (MODEL_GPT) and message history (messages).\n",
    "stream = openai.chat.completions.create(model=MODEL_GPT,messages=messages,\n",
    "#stream=True means the response will come back in chunks — useful for real-time updates.\n",
    "stream=True)\n",
    "#This will accumulate the content as it streams in, chunk by chunk.\n",
    "response =\"\"\n",
    "#This sets up a live-updating output cell in a Jupyter notebook.\n",
    "display_handle = display(Markdown(\"\"),\n",
    "#display_id=True allows you to update the same cell repeatedly instead of printing new ones.\n",
    " display_id=True)\n",
    "\n",
    "\n",
    "#Each chunk contains a partial message.\n",
    "for chunk in stream:\n",
    "    response += chunk.choices[0].delta.content or \"\"\n",
    "#chunk.choices[0].delta.content is the new piece of text.\n",
    "# or \"\" ensures that if the content is None, it doesn’t break the loop.\n",
    "    response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "    #This strips out Markdown code block markers and the word \"markdown\" if it appears — likely to keep the output clean.\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "    #This refreshes the output cell with the latest accumulated content.\n",
    "    #Markdown(response) ensures the formatting is preserved (e.g., bold, italics, headers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62c4a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line sends a chat request to the Ollama API.\n",
    "response = ollama.chat(model=MODEL_LLAMA, messages=messages)\n",
    "\n",
    "\n",
    "#ollama.chat(...): Calls the chat method from the ollama library.\n",
    "# model=MODEL_LLAMA: Specifies which model to use (likely a string like \"llama2\" or \"llama3\").\n",
    "# messages=messages: Passes a list of message dictionaries (e.g., system/user/assistant roles with content) to simulate a conversation history.\n",
    "# Returns: A response object, typically a dictionary containing the model’s reply and metadata.\n",
    "\n",
    "reply = response[\"message\"][\"content\"]\n",
    "# Purpose: Extracts the actual text content from the model’s response.\n",
    "- response[\"message\"]: Accesses the \"message\" key in the response dictionary.\n",
    "- [\"content\"]: Grabs the \"content\" field, which holds the model’s generated reply as a string.\n",
    "- Result: You now have the raw reply text stored in the reply variable.\n",
    "\n",
    "display(Markdown(reply))\n",
    "\n",
    "\n",
    "- \n",
    "\n",
    "display(Markdown(reply))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
